# nanoGPT Training Recipe
#
# Train a character-level GPT model on Shakespeare dataset using Andrej Karpathy's nanoGPT
# Repository: https://github.com/karpathy/nanoGPT
#
# Requirements:
# - Vast.ai account with configured API key
# - GPU instance (RTX 3090 or better recommended)
#
# Training time: ~15-30 minutes for character-level model
#
# Usage:
#   trainsh recipe run nanogpt-train

---
REPO_URL = https://github.com/karpathy/nanoGPT.git
WORKDIR = /workspace/nanoGPT
MODEL_OUT = out-shakespeare
MAX_ITERS = 5000
BLOCK_SIZE = 256
BATCH_SIZE = 64
N_LAYER = 6
N_HEAD = 6
N_EMBD = 384
LOCAL_OUTPUT = ./nanogpt-output
---

# Define GPU host - vast.pick will select from available instances
@gpu = vast

# Select a GPU instance from available Vast.ai machines
> vast.pick @gpu num_gpus=1 min_gpu_ram=16

# Wait for instance to be ready
> vast.wait timeout=5m

# Clone the nanoGPT repository
gpu: git clone ${REPO_URL} ${WORKDIR} 2>/dev/null || (cd ${WORKDIR} && git pull)

# Install dependencies
gpu: pip install torch numpy transformers datasets tiktoken wandb tqdm

# Prepare Shakespeare dataset
gpu: cd ${WORKDIR}/data/shakespeare_char && python prepare.py

# Start training in background (will continue if terminal disconnects)
gpu: cd ${WORKDIR} && nohup python train.py config/train_shakespeare_char.py --block_size=${BLOCK_SIZE} --batch_size=${BATCH_SIZE} --n_layer=${N_LAYER} --n_head=${N_HEAD} --n_embd=${N_EMBD} --max_iters=${MAX_ITERS} --out_dir=${MODEL_OUT} > train.log 2>&1 &

# Poll for training completion
? gpu: "step ${MAX_ITERS}:" timeout=2h

# Test the trained model with sample generation
gpu: cd ${WORKDIR} && python sample.py --out_dir=${MODEL_OUT} --num_samples=1 --max_new_tokens=200

# Download results
gpu:${WORKDIR}/${MODEL_OUT} -> ${LOCAL_OUTPUT}/model
gpu:${WORKDIR}/train.log -> ${LOCAL_OUTPUT}/train.log

# Stop the instance to save costs
> vast.stop
